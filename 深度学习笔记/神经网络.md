***神经网络***

1. 人工神经元：人类神经元中抽象出来的数学模型。

   ​	Inputs -> Operations -> Outputs

2. 单层感知机：Inputs -> Outputs

   ​	只能解决线性分类任务，与逻辑回归类似。

   ​	感知机致命缺点：无法解决异或问题。（感知机是一条直线，在平面上分成两部分，无法正确判断异或）。

3. 多层感知机(MLP)：Inputs -> Hidden -> Outputs

   ​	单层神经网络基础上引入一个或多个隐藏层，使神经网络有多个网络层，因而得名多层感知机。

   ​	隐藏层可以有多个，有几层就是几层感知机。

   ​	权重矩阵（行：输入 。列：输出） 

   ​	多层感知机的关键问题在于如何训练其中各层间的连接权值。（使用**反向传播** 算法）

   ​	无激活函数，网络退化为单层网络。

   ​	隐藏层加入激活函数，可便面网络退化。

4. **反向传播机制：解决神经网络模型的高效率优化问题。**

5. 无监督预训练：解决深度模型初始化、训练不稳定的问题。

6. **激活函数：缓解深层模型的梯度消失问题。**

   1. 让多层感知机成为真正的多层，否则等价于一层。
   2. 引入非线性，使网络可以逼近任意非线性函数（万能逼近定理）
   3. 激活函数需要**连续并可导**（少数点可以不可导）、激活函数和导函数要**简单**、激活函数的导函数的**值域**要在合适区间内。
   4. 常见的激活函数：Sigmoid函数（S型）、Tanh（双曲正切）、ReLu（修正线性单元）。前两者有饱和区，最后一种没有，更优一些。
   5. 一旦达到饱和区，就不利于深层模型的优化。

7. 前向传播：输入层数据开始从前向后，数据逐步传递至输出层。

8. 后向传播：**损失函数**开始从后向前，**梯度**逐步传递至第一层

   1. 作用：用于权重更新，使网络输出更接近标签。
   2. 原理：微积分中链式求导法则。 (多元函数求导)
   3. 损失函数：衡量模型输出与真实标签的差异。值越小，表现越好。

9. 梯度下降法：权重沿梯度**负方向更新** ，是函数值减少。

   1. 反向传播之后，求取的每个变量的梯度，使用梯度下降法去更新权重。
   2. 导数：函数在指定坐标轴上的变化率。
   3. 方向导数：指定方向上的变化率。
   4. 梯度：一个向量，方向为方向导数取得最大值的方向。
   5. 学习率：控制更新步长。（通常小于1）

10. 损失函数：衡量模型输出与真实的标签之间的差距。

   11. 损失函数：单样本差异值。Loss（越小越好）

   12. 代价函数：求总体样本的Loss的平均值。Cost（越小越好）

   13. 目标函数：Obj = Cost + 正则项 （正则项：控制模型复杂度，模型过于复杂，会导致过拟合现象）

   14. 常见的损失函数：

       1. MSE均方误差：在回归任务中使用

       2. CE交叉熵：在分类任务中使用（概率）

          1. 信息熵：描述信息的不确定度。（所有可能取值的信息量的期望）
          2. 相对熵：又称K-L散度，衡量两个分布之间的差异。

          **交叉熵 = 信息熵 + 相对熵**

          **优化交叉熵等价于优化相对熵**

       3. 交叉熵：衡量两个概率分布的差异。

          概率值是非负的

          概率之和等于1

       4. 交叉熵的好伙伴---Softmax函数：将数据变换到符合概率分布的形式。

       5. Softmax函数：

          操作：

                 	1. 取指数，实现非负数。e的x次方（x为数据值）
                       2. 除以指数之和，实现之和为1。

   15. 权值初始化：训练前对权值参数赋值，良好的权值初始化有利于模型训练。（如果权值都为0，就相当于每一层网络层，都相当于一个神经元。）

       1. 随机初始化法：高斯分布随机初始化，从高斯分布中随机采样，对权重进行赋值，比如N~(0,0.1²)
       2. 3σ准则：数值分布在（u-3σ,u+3σ）中的概率为99.73%（权重不能太大，必须在一定范围内）
       3. 标准差：可以使权重在一定的范围内。
          1. 自适应标准差：自适应方法随机分布中的标准差。
             1. Xavier初始化（a是输入神经元的个数，b是输出神经元的个数）
             2. Kaiming初始化（==MSRA方法）

   16. 正则化方法：**减小方差**的策略，通俗理解为**减轻过拟合**

       的策略。

       1. 误差 = 偏差 + 方差 + 噪声
       2. 偏差：学习算法本身的拟合能力
       3. 方差：数据扰动所造成的影响
       4. 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界。

       正则化方法：L1、L2

              	1. L1

       ​

       2.  L2（weight decay）权值衰减：正则项

       3.  Dropout：随机失活

          4.  优点：避免过渡依赖某个神经元，实现减轻过拟合
          5.  随机：以一定的概率选中神经元，这些神经元会失去活性。
          6.  失活：失去活性，权重变为0

          **注意**：在使用Dropout时，训练和测试两个阶段的**数据尺度变化**。训练时，会是每一层几个神经元失去活性。测试时，所有神经元都会起作用。（所以测试时，神经元输出值需要乘以P概率）。

***CNN卷积神经网络***: 无监督学习

1. 发展史：CNN是针对**图像** 领域任务提出的神经网络。（图像任务：图像分类、图像分割、目标检测、图像检索）

   1. CNN主要包括：卷积层、池化层、全连接层。

      1. 卷积层：用于对图像进行特征提取。（卷积核、步长、填充）
      2. 池化层：降低特征图大小，降低后续操作的计算量和参数量。
      3. 全连接层：最终进行分类输出，本质就是多层感知机。

   2. 第一个卷积神经网络雏形 ： 新认知机

      缺点：没有反向传播算法更新权值，模型性能有限。

   3. 第一个大规模商用卷积神经网络：Lenet-5

      缺点：无大量数据和高性能计算资源。

   4. 第一个技惊四座的卷积神经网络：AlexNet

      成功要素：

      ​	算料：ImageNet

      ​	算力：GPU

      ​	算法：AlexNet

2. 卷积层

   1. 图像识别特点：

      1. 特征具有局部性（老虎的重要特征“王”在局部头部）
      2. 特征可能出现在任何位置（王字可能出现在图片的任何位置）
      3. 下采样图像，不会改变图像目标（图像分辨率的下降和改变，不会改变图像类别）

   2. 卷积层特点：

      1. 特征具有局部性：卷积核每次仅连接K*K（卷积核尺寸）区域。

         全连接

         局部连接

      2. 特征可能出现在任何位置：卷积核参数重复使用（参数共享），在图像上滑动。

      3. **卷积核** ：具可学习参数的算子，用于对输入图像进行**特征提取** ，输出通常称为**特征图**。

         边缘检测（卷积核=0）、锐化（=1）、高斯模糊（从中心往外侧值越小）

      4. **填充** （Padding）:在输入图像的周围添加额外的行\列（外围加一圈0）

         作用：

                	1. 使卷积后**图像分辨率不变** ，方便计算特征图尺寸的变化。
                   	2. 弥补边界信息的“丢失”

      5. 步幅（Stride）：卷积核滑动的行数和列数称为步幅，控制输出特征图的大小，会被缩小1/s倍

      6. 特征图尺寸计算。

      7. 多通道卷积。

3. 池化层

   1. 池化：一个像素表示一块区域的像素值，降低预想分辨率。（**使特征图变小，简化网络计算复杂度。压缩特征，提取主要特征** ）
   2. 一块区域像素如何被一个像素代替：
      1. Max Pooling，取最大值
      2. Average Pooling 取平均值 
   3. 可以使用步长为2的卷积来代替池化（现在不怎么使用池化）。
   4. 池化作用
      1. 缓解卷积层对位置的过度敏感
      2. 减少冗余
      3. 降低图像分辨率，从而减少参数量

4. Lenet-5

   1. **一个卷积核只输出一个通道** 。

5. 卷积和池化输出尺寸计算

   ​	FinFin 是输入图像、k 是卷积核的大小、p 是图像填充的大小、s 是卷积核的步幅、FoFo 是输出、⌊6.6⌋⌊6.6⌋ 是向下取整的意思，比如结果是 6.6，那么向下取整就是 6

   Fo=⌊Fin −k+2p⌋/2+1

6. 经典网络：

   1. 如何判断一个网络架构有多少层？

      卷积层和全连接层的总数。

      因为带有参数

   2. VGG：在pooling完之后，会损失一些特征信息，体积变为原来的四分之一。为了弥补损失，在下一次卷积中使得特征图体积翻倍。

   3. Resnet：深层网络遇到的问题，当层数太多，超过20层之后，表现效果反倒没有太好，典型的是VGG16层，20层之后，表现变差。

      ​	**思想** ：在更深的层时，如果从21层到22层，通过卷积发现学习的不好，将会给23层传入21层与22层学习后的结果。比如21层输出x，22层卷积后为F(x)，所以给23层输入的时候为H(x) = F(x) + x。假如：22层学习的非常不好， 则给22层的权值为0 ，使得23层输入时，为x。保底也是21层输出结果。

      ​	**网络加深后，至少不比原来网络差** 

***RNN循环神经网络***

1. RNN会将前一层得到的结果保留，参与下一次的运算。

2. 序列数据：是常见的数据类型，前后数据通常具有关联性。

3. 语言模型：是自然语言处理NLP重要技术。

4. RNN:是针对序列数据而生的神经网络结构，核心在于循环使用网络层参数，避免时间步增大带来的参数激增，并引入**隐藏状态** 用于记录历史信息，有效的处理数据的前后关联性。

5. 隐藏状态（Hidden state）：用于记录历史信息，有效处理数据的前后关联性，激活函数采用Tanh,将输出值域限制在（-1,1），防止数值呈指数级变化。

6. RNN特性：

   1. 循环神经网络的隐藏状态可以捕捉截止当前时间步的序列的历史信息。
   2. 循环神经网络模型参数的数量不随时间步的增加而增长。

7. 循环使用权重矩阵。

8. RNN的通过（穿越）时间反向传播：

   ​	缺点：梯度随时间t呈指数变化，易引发**梯度消失** 或**梯度爆炸**

9. 门控循环网络：

   GRU:

   1. 缓解RNN梯度消失带来的问题，引入门概念，来控制信息流动，使模型更好的记住长远时期的信息，并缓解梯度消失。

   2. 重置门：哪些信息需要遗忘（用于遗忘上一时间步隐藏状态）

   3. 更新门：哪些信息需要注意（用于更新当前时间步隐藏状态）

   4. 激活函数使用：Sigmoid,值域为（0,1）0表示遗忘，1表示保留。

   5. 候选隐藏状态：输入与上一时间步隐藏状态共同计算得到候选隐藏状态，用于隐藏状态计算。

      ​	通过重置门，对上一事件隐藏状态进行选择性遗忘，对历史信息更好的选择。

   LSTM：长短期记忆网络

   1. 遗忘门：那些信息需要遗忘
   2. 输入门：那些信息需要流入当前记忆细胞
   3. 输出门：那些记忆信息流入隐藏状态
   4. 记忆细胞：特殊的隐藏状态，记忆历史信息（特殊隐藏状态，存储历史时刻信息）。

***GANs生成对抗网络***



***RL深度强化学习***



***GNN图神经网络***



***GCN图卷积神经网络***

