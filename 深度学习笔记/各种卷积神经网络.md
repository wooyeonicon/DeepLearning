# ***LeNet*** 



8层网络、输入层、3个卷积层、2个池化层、1个全连接层、输出层

所有卷积核5*5、步长strid=1、池化层Averagepooling、激活函数Sigmoid、没有padding

sigmoid会出现梯度消失。

大部分参数还是在全连接层。

激活函数sigmoid、adam优化器

(LeNet-5激活函数为ReLU)

softmax做概率归一化

**接受输入图片大小32x32，输出10个类别。**

|       | 输入       | 卷积、池化、神经元          | 输出                     | 训练参数                        |
| ----- | -------- | ------------------ | ---------------------- | --------------------------- |
| 输入层   |          |                    | 32x32                  | 0                           |
| 卷积层1  | 1x32x32  | 6个5x5卷积核 stride=1  | 6x28x28   1+(32-5+0)/1 | 5x5x6+6=156   加的6为偏置项       |
| 池化层1  | 6x28x28  | 2x2  stride=2      | 6x14x14                | 0                           |
| 卷积层2  | 6x14x14  | 16个5x5卷积核 stride=1 | 16x10x10               | 6x5x5x16+16=2416   6为输入卷积核数 |
| 池化层2  | 16x10x10 | 2x2  stride=2      | 16x5x5                 | 0                           |
| 全连接层1 | 16x5x5   | 120个5x5卷积核         | 120x1x1                | 16x5x5x120+120=48120        |
| 全连接层2 | 120x1x1  |                    | 84                     | 120x1x1x84+84               |
| 输出层   | 84x1x1   | softmax            | 10                     | 84x10+10                    |

​

# ***AlexNet*** 

激活函数ReLU(模型收敛快，避免梯度消失)

计算简单，运算速度快。

随机丢弃（dropout）：防止过拟合。每次训练都随机让一定的神经元停止参与运算，增加模型的泛化能力，稳定性和鲁棒性。

12层网络模型：输入层、5个卷积层、3个池化层、3个全连接层、激活函数ReLu、输出层

maxpooling池化

​

|       | 输入                   | 卷积、池化、神经元                              | 输出        | 参数                |
| ----- | -------------------- | -------------------------------------- | --------- | ----------------- |
| 输入层   | 224x224x3            | padding(1,2)                           | 227x227x3 | 0                 |
| 卷积层1  | 227x227x3            | 48个11x11卷积核  stride=4 ReLU             | 48x55x55  | 3x48x11x11+48     |
| 池化层1  | 48x55x55             | 3x3 stride=2                           | 48x27x27  | 0                 |
| 卷积层2  | 48x27x27             | 128个5x5卷积核 stride=1 ReLU  padding(2,2) | 128*27x27 | 48x128x5x5+128    |
| 池化层2  | 128x27x27            | 3x3 stride=2                           | 128x13x13 | 0                 |
| 卷积层3  | 128x13x13            | 192个3x3卷积核 stride=1 ReLU padding(1,1)  | 192x13x13 | 128x192x3x3+192   |
| 卷积层4  | 192x13x13            | 192个3x3卷积核 stride=1 ReLU padding(1,1)  | 192x13x13 | 192x192x3x3+192   |
| 卷积层5  | 192x13x13            | 128个3x3卷积核 stride=1 ReLU padding=(1,1) | 128x13x13 | 192x128x13x13+128 |
| 池化层3  | 128x13x13            | 3x3 stride=2                           | 128x6x6   | 0                 |
| 全连接层1 | 128x6x6(dropout=0.5) | 2048  ReLU                             | 2048      | 128x6x6x2048+2048 |
| 全连接层2 | 2048(dropout=0.5)    | 2048 ReLU                              | 2048      | 2048x2048+2048    |
| 输出层   | 2048                 | 5  softmax                             | 5         | 2048x5+5          |



# ***VGG*** 

卷积核3x3、池化尺寸2x2

堆叠两个3x3卷积核替代一个5x5卷积核

堆叠三个3x3卷积核替代一个7x7卷积核

相同感受视野，训练参数量减少。

VGG11有8层卷积、VGG16有13层卷积、VGG19有16层卷积，全连接层都是三层（全连接1、全连接2、输出层）

 不同于AlexNet使用11*11、5*5的卷积核，**VGG网络的一大特点就是它的卷积全部是用3\*3的卷积核来做**。

因为在实验中发现使用多个3x3的卷积核就可以达到与5x*5或11*x11这样的大卷积核一样的感受野；此外还顺便增加了网络的深度，增加了使用非线性层来提取特征的次数（卷积的过程越多，特征提取越细致），提高了模型的学习能力；最重要的是，这样做参数还会更少。(使用多个3x3卷积核可以得到相同效果、增加网络深度、增加提取特征的次数、参数更少)。

VGG还有一个重要的操作，就是池化之后，为了弥补信息的损失，会通过下一次卷积来使通道数翻倍。比如某一次池化之后我们得到了128个特征图，下一步我们直接使用256个卷积核进行卷积。

![img](https://img-blog.csdnimg.cn/20200813185452561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NZU196eGN2Ym5t,size_16,color_FFFFFF,t_70#pic_center)

# **GoogleNet**

1. 最后的全连接层都换成了1x1的卷积层，大大加速了训练速率。

2. 增大网络的缺点：参数太多，容易过拟合。网络计算复杂度越大。网络越深，梯度越往后越容易消失。

3. 针对缺点解决方式：增加网络深度和宽度的同时减少参数。

4. Inception结构的主要思路：用密集成分来近似最优的局部稀疏结构。

5. 采用不同大小的卷积核，1x1 3x3 5x5（为了方便对齐）,只需要设定各自的padding=0,1,2,卷积后就可以得到相同维度的特征，再直接拼接。

6. 参数计算：上一层输出100x100x128  经过256x5x5(stride=1,padding=2)，输出100x100x256。卷积层参数128x5x5x256。假如上层输出先经过32x1x1卷积，再经过256x5x5卷积，最终输出数据任然是100x100x256,但卷积参数为128x32x1x1+32x256x5x5大概减少四倍。

7. Inception v1参数少效果好：模型层数深，去除了全连接层（因为全连接层需要大量参数）。设计Inception Module提高参数的利用率。

8. Inception Module:四个分支，小学生才选择，我全要。

   （1x1卷积是为了改变输入的通道数）

   1. **1x1卷积：降维处理。**
   2. 先1x1卷积再3x3卷积padding=1
   3. 先1x1卷积再5x5卷积padding=2
   4. 3x3最大池化后直接使用1x1卷积

   （最后将四个分支合并，高宽都相同，通道数会增加。）

   **深度拼接depth concat**

   **输出Average Pooling**

   **3个输出 6:2:2(权重)  多个loss**

   SeNet CBM

      ​

   9. GoogleNet采用平均池化来代替全连接层。但实际上最后一层还是加了一个全连接层，主要为了方便之后的微调。虽然取消了全连接层，但还是使用Dropout。为了避免梯度消失，网络额外增加了2个softmax用来前向传到梯度。

   10. 卷积层参数：输入通道x输出通道xkernel

   11. 5段，9个Inception块

12. 批量归一化

    1.明确问题：在forward与backward的时候，网络越深，梯度会越大，网络越接近输入，梯度越小。在backward过程中会出现：网络层接近输出的地方，参数变动大，参数会快速收敛，但是在网络层接近输入的地方，参数收敛会很慢，因为梯度太小。（**自己理解：上层收敛快，下层收敛慢，当上层参数训练好的话，传到下层，参数训练收敛到下层的时候，上层又改变了，又得重新训练，导致训练过慢。**）

    ​	损失出现在最后，后面的层训练较快，很快收敛。

    ​	数据在最底部。（反向传播，训练：改变参数）

    ​		底部的层训练较慢。

    ​		底部层一变化，所有的层都得跟着变化

    ​		最后的那些层需要重新学习多次

    ​		导致整体收敛变慢

    2. 解决问题：可以在学习底部层的时候避免变化顶部层。

       ​	固定小批量里面的均值和方差。

       ​	作用在全连接层和卷积层输出上，激活函数前。

       ​	作用在全连接层和卷积层输入上。

       ​		对全连接层，作用在特征维

       ​		对卷积层，作用在通道维。

    3. 批量归一化：可能通过在每个小批量里加入噪音来控制模型复杂度。（没必要与丢弃法混合使用）

    4. 总结：批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放。（可以加速收敛速度，但一般不改变模型的精度）。



# ResNet残差网络

1. 加更多的层总是改进精度吗？不是
2. ResNet是加更多的层至少不让模型精度变差。
3. **残差块** ：f(x) = x + g(x)
4. 可以把残差块放在卷积层、激活层、或者batch normalization层
5. 总结：残差块使得很深的网络更加容易训练，甚至可以训练一千层的网络。

**不用ResNet做检测。**

**衍生版**

网络改进、实验设计（怎么做的网络）、行文结构（新名词）

HRNet

# **支持向量机**

用于回归、分类问题。

https://blog.csdn.net/weixin_44692890/article/details/120248409?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-120248409.142^v50^control_1,201^v3^control_2&spm=1018.2226.3001.4187

通俗理解：SVM就是试图把棍放在最佳位置，好让在棍两边有尽可能大的间隙。

**线性不可分** ：在二维平面中两种小球很混乱。

**如何解决？** 拍桌子，一种球飞起来，将一个平面插入（**低维线性不可分问题转化到高维**），从空中看这些球，看起来被一条曲线分开。**Cover定理** 

球：data数据、棍子：classifier分类器、找最大间隙的方法:optimization最优化、拍桌子：kernelling核、纸:hyperpalne（分类超平面）

**为什么找最大间隙？** 样本发生微小变化，不会影响分类结果。所以该线对样本的“容忍性”好，为决策边界。

**支持向量：** 与b11和b12相交的样本点叫支持向量。


1. 深度学习解读（深度学习知识点全面总结csdn）

   **前向传播**:就是给网络输入一个样本向量，该样本向量的各元素，经过各隐藏层的逐级加权求和+非线性激活，最终由输出层输出一个预测向量的过程。

   **线性激活函数** ：梯度相同，无法通过训练将模型训练更准确。在回归问题中，或者输出层使用

   **非线性激活函数**:梯度变化，不是一条直线。

   **激活函数** ：增强神经网络模型的非线性，没有激活函数的每层都相当于矩阵相乘。



总结：

1、池化的好处是为了降低空间尺寸，减少网络中的参数，节省计算资源。

2、通过卷积、激活、池化得到参数较少并保持原有特征信息的特征图，将特征图展平进入神经网络（全连接层），得到预测概率，通过反向传播的方式不断调整这些参数，已达到预测值与真实值最接近的情况，完成网络的训练。

3、卷积后尺寸 = 1+（输入-卷积核+加边像素数）/ 步长

4、计算参数(模型训练中不断修改的内容，最终找到预测值与真实值损失值最小的那些权重)：

​	卷积层：（卷积参数+偏置参数）x 卷积核个数

​	全连接层：神经元连接权重+偏置参数

5、感受视野：输出层一个元素对应输入层区域的大小。

​	计算：感受视野 = （上一层感受视野 - 1)x步长+卷积核尺寸

线性可分问题不可分

sigmoid 函数的求导=sigmoid*（1-sigmoid）

正态分布函数的累计

反向传播在卷积上是怎么计算的  

Google 

Resnet

梯度消失，梯度爆炸推论

svm 逻辑回归 正则化

Batch Nromalization



流模型



from torchvision 

找一个最新网络参考，不要用官方权重，

# **onnx** 

用来**部署模型** ，将Pytorch模型转换为ONNX模型，只需要torch.onnx.export

1. torch.onnx.export将普通pytorch模型转换成torchScript模型，有两种方法。
   1. trace追踪法：导出计算图的方法（默认），循环模型由于n不同，模型不同。生成静态图。
   2. script:记录法。循环模型由于n不同，模型相同。
2. **参数**
   1. model:模型
   2. args:模型输入
   3. f:导出的onnx文件名
   4. export_params：模型是否存储模型权重。（中间表示包含：模型结构和模型权重），onnx用同一个文件距离模型的结构和权重。**一般默认参数为True**，如果用来在不同框架间传递模型（pytorch到tensorflow）而不是用于部署，则可以设置参数为False。
   5. input_names和output_names：设置输入和输出张量的名称。在部署时，保证onnx和推理引擎中使用同一套名称。
   6. opset_version：转换时参考哪个onnx算子集版本，默认为9。
   7. dynamic_axes：指定输入输出张量的哪些维度是动态的。（onnx默认所有参与运算的张量都是静态的，但是在现实中，又希望模型的输入张量是动态的。）（onnx要求每个动态维度都有一个名字）